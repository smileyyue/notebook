[TOC]

# 数据结构

## 哈希表

### 参考链接：

- [wiki百科](https://zh.wikipedia.org/wiki/%E5%93%88%E5%B8%8C%E8%A1%A8)
- [哈希表和哈希桶](https://blog.csdn.net/wm12345645/article/details/80329944)
- [hash函数应用（整理）](https://blog.csdn.net/szu_tanglanting/article/details/12406605)

### 定义

#### 散列表（Hash table，也叫哈希表）

哈希表是根据键（Key）而直接访问在内存存储位置的数据结构。也就是说，它通过计算一个关于键值的函数，将所需查询的数据映射到表中一个位置来访问记录，这加快了查找速度。这个映射函数称做散列函数，存放记录的数组称做散列表。

>  即通过key值和哈希函数hash()，得到存放value的目标地址addr=hash(key)，这样(key,value)的value就存放在了addr指向的地址中。

#### 冲突(Collision)

$k_1 \ne k_2​$但$f(k_1) = f(k_2)​$ 则发生了冲突，两个不同的key指向同一个地址。

#### 载荷因子

散列表的载荷因子定义为：$\alpha= \frac{填入表中的元素个数}{散列表的长度}$

$\alpha​$是散列表装满程度的标志因子。由于表长是定值，$ \alpha​$与“填入表中的元素个数”成正比，所以，$\alpha​$越大，表明填入表中的元素越多，产生冲突的可能性就越大；反之，$\alpha ​$越小，标明填入表中的元素越少，产生冲突的可能性就越小。实际上，散列表的平均查找长度是载荷因子$\alpha​$的函数，只是不同处理冲突的方法有不同的函数。

对于开放定址法，荷载因子是特别重要因素，应严格限制在0.7-0.8以下。超过0.8，查表时的CPU缓存不命中（cache missing）按照指数曲线上升。因此，一些采用开放定址法的hash库，如Java的系统库限制了荷载因子为0.75，超过此值将resize散列表。

### 散列函数构造方法

- 直接定址法：取关键字或关键字的某个线性函数值为散列地址。即$hash(k)=k​$或$hash(k)=a\cdot k+b​$，其中$a\,b​$为常数（这种散列函数叫做自身函数）
- 数字分析法：假设关键字是以r为基的数，并且哈希表中可能出现的关键字都是事先知道的，则可取关键字的若干数位组成哈希地址。
- 平方取中法：取关键字平方后的中间几位为哈希地址。通常在选定哈希函数时不一定能知道关键字的全部情况，取其中的哪几位也不一定合适，而一个数平方后的中间几位数和数的每一位都相关，由此使随机分布的关键字得到的哈希地址也是随机的。取的位数由表长决定。
- 折叠法：将关键字分割成位数相同的几部分（最后一部分的位数可以不同），然后取这几部分的叠加和（舍去进位）作为哈希地址。
- 随机数法
- 除留余数法：取关键字被某个不大于散列表表长m的数p除后所得的余数为散列地址。即$hash(k)=k \mod p,p\leq m$。不仅可以对关键字直接取模，也可在折叠法、平方取中法等运算之后取模。对p的选择很重要，一般取素数或m，若p选择不好，容易产生冲突。

### 冲突处理方法

- 开放定址法：线性探测和平方探测，这种探测会有时探测到一连串被用过的空间，这串空间被称作：聚集(Cluster，又称堆积)，聚集会造成开放定址法的性能灾难性损失，浪费时间。
- 单独链表法(又称哈希桶，hash slot）：**常用**，将散列到同一个存储位置的所有元素保存在一个链表中。实现时，一种策略是散列表同一位置的所有冲突结果都是用栈存放的，新元素被插入到表的前端还是后端完全取决于怎样方便。一个链表被称作一个桶

- 双散列。

- 再散列：$hash{i}=hash{i}(key), i=1,2...k$。$hash{i}$是一些散列函数。即在上次散列计算发生冲突时，利用该次冲突的散列函数地址产生新的散列函数地址，直到冲突不再发生。这种方法不易产生“聚集”（Cluster），但增加了计算时间。

- 建立一个公共溢出区

> 一般使用素数作为除数等用来计算，素数的冲突概率小很多

### 查找效率

查找过程中，关键码的比较次数，取决于产生冲突的多少，产生的冲突少，查找效率就高，产生的冲突多，查找效率就低。因此，影响产生冲突多少的因素，也就是影响查找效率的因素。影响产生冲突多少有以下三个因素：

1. 散列函数是否均匀；
2. 处理冲突的方法；
3. 散列表的载荷因子（英语：load factor）。

### hash函数优劣评估指标

1. 散列分布性：即桶的使用率backet_usage = (已使用桶数) / (总的桶数)，这个比例越高，说明分布性良好，是好的hash设计。
2. 平均桶长：即avg_backet_len，所有已使用桶的平均长度。理想状态下这个值应该=1，越小说明冲突发生地越少，是好的hash设计。

### 例程

#### 参考文献2例程节选

hash表结构体：

``` cpp
enum State//每个位置都有自己的状态
{
   EXIST,//存在的
   EMPTY,//可用的
   DELETE//被删除的
};
 
typedef struct KVP
{
  DataType value;//存储的值
  enum State state;//位置的状态
  int order;//顺序
  
}KVP;
 
 
typedef struct HashTable
{
   KVP *array;
   int  size;//有效容量
   int capacity;//容量
   int total;//已用容量（包括EXIST,DELETE）
   explore exp;//函数指针
}HashTable;
```

哈希函数：采用除数余留法，除数是哈希表的容量，而容量则选择素数

``` cpp
//int哈希函数
int HashFunc(int  capacity,DataType data)
{
	return  data%capacity;
}
//字符哈希函数
static size_t BKDRHash(const char * str)
{
    unsigned int seed = 131; // 31 131 1313 13131 131313
    unsigned int hash = 0;
    while (*str )
    {
        hash = hash * seed + (*str++);
    }
    return hash;
}
```

扩容函数：

``` cpp
//扩容
void AddCapacity(HashTable *ph)
{
  int i=0;
 
  HashTable NewHt;
  int NewN;
  assert(ph);
  if(ph->total*10/ph->capacity>=7)//乘10 是为了避免浮点数的问题，当已用元素个数占用总元素个数的70%以上时
  {
	  NewN=GetNextPrimeNum(ph->capacity);//这个函数的作用是：得到离ph->capacity最近的一个素数，一般比它大
	  InitHashTable(&NewHt,doub,NewN);
	  for(;i<ph->capacity;i++)
	  {
		  if(ph->array[i].state==EXIST)
		  {
			  InsertHashTable(&NewHt,ph->array[i].value);
		  }
	  }
	  swap(&NewHt,ph);
	  DestroyHashTable (&NewHt);
 
  }
}

void AddCapacity(HashTable *ph)
{
  int i=0;
 
  HashTable NewHt;
  int NewN;
  assert(ph);
  if(ph->total*10/ph->capacity>=7)//乘10 是为了避免浮点数的问题，当已用元素个数占用总元素个数的70%以上时
  {
	  NewN=GetNextPrimeNum(ph->capacity);//这个函数的作用是：得到离ph->capacity最近的一个素数，一般比它大
	  InitHashTable(&NewHt,doub,NewN);
	  for(;i<ph->capacity;i++)
	  {
		  if(ph->array[i].state==EXIST)
		  {
			  InsertHashTable(&NewHt,ph->array[i].value);
		  }
	  }
	  swap(&NewHt,ph);
	  DestroyHashTable (&NewHt);

  }
}
```

容量选择（即选择合适的素数）：

``` cpp
//得到最近的一个素数
int GetNextPrimeNum(int cur)
{
  int i=0;
  const int _PrimeSize=28;
  static const  long _PrimeList [28] =
 {
	 10ul, 97ul, 193ul, 389ul, 769ul,
	 1543ul, 3079ul, 6151ul, 12289ul, 24593ul,
	 49157ul, 98317ul, 196613ul, 393241ul, 786433ul,
	 1572869ul, 3145739ul, 6291469ul, 12582917ul, 25165843ul,
	 50331653ul, 100663319ul, 201326611ul, 402653189ul, 805306457ul,
	 1610612741ul, 3221225473ul, 4294967291ul
 }; 
  for(;i<_PrimeSize;i++)
  {
	  if(_PrimeList[i]>cur)//是否可以相等
	  {
	    return _PrimeList[i];
	  }
  }	  
  return _PrimeList [_PrimeSize-1];
```

#### linux hash实现

hash_long在<linux/hash.h>中定义如下:

``` c
/* 2^31 + 2^29 - 2^25 + 2^22 - 2^19 - 2^16 + 1 */
#define GOLDEN_RATIO_PRIME_32 0x9e370001UL
/*  2^63 + 2^61 - 2^57 + 2^54 - 2^51 - 2^18 + 1 */
#define GOLDEN_RATIO_PRIME_64 0x9e37fffffffc0001UL
 
#if BITS_PER_LONG == 32
#define GOLDEN_RATIO_PRIME GOLDEN_RATIO_PRIME_32
#define hash_long(val, bits) hash_32(val, bits)
#elif BITS_PER_LONG == 64
#define hash_long(val, bits) hash_64(val, bits)
#define GOLDEN_RATIO_PRIME GOLDEN_RATIO_PRIME_64
#else
#error Wordsize not 32 or 64
#endif
 
static inline u64 hash_64(u64 val, unsigned int bits)
{
    u64 hash = val;
 
    /*  Sigh, gcc can't optimise this alone like it does for 32 bits. */
    u64 n = hash;
    n <<= 18;
    hash -= n;
    n <<= 33;
    hash -= n;
    n <<= 3;
    hash += n;
    n <<= 3;
    hash -= n;
    n <<= 4;
    hash += n;
    n <<= 2;
    hash += n;
 
    /* High bits are more random, so use them. */
    return hash >> (64 - bits);
}
 
static inline u32 hash_32(u32 val, unsigned int bits)
{
    /* On some cpus multiply is faster, on others gcc will do shifts */
    u32 hash = val * GOLDEN_RATIO_PRIME_32;
 
    /* High bits are more random, so use them. */
    return hash >> (32 - bits);
}
 
static inline unsigned long hash_ptr(const void *ptr, unsigned int bits)
{
    return hash_long((unsigned long)ptr, bits);
}
#endif /* _LINUX_HASH_H */
```

首先，hash的方式是，让key乘以一个大数，于是结果溢出，就把留在32/64位变量中的值作为hash值，又由于散列表的索引长度有限，我们就取这hash值的高几为作为索引值，之所以取高几位，是因为高位的数更具有随机性，能够减少所谓“冲突”。什么是冲突呢？从上面的算法来看，key和hash值并不是一一对应的。有可能两个key算出来得到同一个hash值，这就称为“冲突”。

那么，乘以的这个大数应该是多少呢？从上面的代码来看，32位系统中这个数是0x9e370001UL，64位系统中这个数是0x9e37fffffffc0001UL。这个数是怎么得到的呢？

>  这个值接近黄金分割术会使得hash的结果最好，实际中linux内核选取的这两个值都是接近黄金分割数并且容易计算的。

“Knuth建议，要得到满意的结果，对于32位机器，2^32做黄金分割，这个大数是最接近黄金分割点的素数，0x9e370001UL就是接近 2^32*(sqrt(5)-1)/2 的一个素数，且这个数可以很方便地通过加运算和位移运算得到，因为它等于2^31 + 2^29 - 2^25 + 2^22 - 2^19 - 2^16 + 1。对于64位系统，这个数是0x9e37fffffffc0001UL，同样有2^63 + 2^61 - 2^57 + 2^54 - 2^51 - 2^18 + 1。”

从程序中可以看到，对于32位系统计算hash值是直接用的乘法，因为gcc在编译时会自动优化算法。而对于64位系统，gcc似乎没有类似的优化，所以用的是位移运算和加运算来计算。首先n=hash, 然后n左移18位，hash-=n，这样hash = hash * (1 - 2^18)，下一项是-2^51，而n之前已经左移过18位了，所以只需要再左移33位，于是有n <<= 33，依次类推，最终算出了hash值。

# 算法

##  查找与排序

### 二分查找

适用范围：在已排序的数组中查找（部分排序也可，例如旋转数组）

时间复杂度：O(logn)

注意事项：

- mid = start + (end-start)/2;  等价于 mid = (start+end)/2;  用第二种较好
- mid已经判断过了就不要重复判断了，`start = mid + 1; end = mid - 1;`，以免陷入死循环
- 终止条件为while(start<end);
- 使用闭区间[start, end]，否则如果最终没找到停在开区间的边界，会造成数组越界；

## 动态规划

本质是对递归回溯的优化，递归回溯是从上到下的，动态规划是从下到上的，并且通过额外的内存空间记录已经计算过的值，保留做以后用，同时比较局部的各值，只留下最优的用于之后的比较。

动态规划的使用前提：

1. 是一个求最优解的问题；
2. 可以分成若干个更小的求最优解的问题，若干个更小的最优解中最优的一个是大问题的最优解；
3. 小问题之间还有相互重叠的更小子问题；
4. DAG的无环路径问题；

动态规划的思考方法：

1. 从上往下分析，从下往上求解；
2. 可以选个小的问题，自己推一推试试，记录中间变量；
3. 思考大问题能如何划分成小问题，然后再去考虑状态转移方程；

动态规划的时间复杂度：

- 线性问题的时间复杂度为$O(n)​$(d[i]仅与d[i-1]或d[i-1]和d[i-2]等有限个前量有关)，  $O(n^2)​$（d[i]与d[i-1]~d[1]全体有关，此时要做第二层循环for i=1~i-1，所以是$n^2​$）

## 贪心算法

贪心算法和动态规划的区别：

- 动态规划是把大问题分成小问题，然后根据各个小问题最优解得出大问题的最优解，如果小问题的最优解不是大问题的最优解，会找其他小问题的解，找到大问题的最优解。
- 而贪心算法是直接找最小问题的局部最优解，并直接把局部最优解当作全局最优解去继续求解。

例如：3， 5，10，50，100元纸币中挑出285元，则贪心算法直接从最大的找满，继续往下找到3元为止，结果是$100*2+50*1+10*3+5*1=285$，这是对的，但如果挑出286元，则贪心算法结果也是$100*2+50*1+10*3+5*1=285$，答案是错的，最后应该用两张3元，而不是一张5元。如果最小面额是1元则可以用贪心算法，最小是3就要用动态规划了。

> 贪心算法可以用来求近似最优解，因为贪心算法的时间复杂度和空间复杂度都是$O(1)$；
>
> 使用贪心算法时，一定要通过数学证明局部最优解是全局最优解，才能使用。